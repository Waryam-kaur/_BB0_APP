# -*- coding: utf-8 -*-
"""EDA_&_FEATURE_ENGINEERING_NEW_DATA_2_ (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11nATK0ADLhw1fu8VD0kSMXDOKfdnIgsM
"""

# Loading the important libraries
import pandas as pd
import numpy as np

# settking the file path
file_path = "SawWhets_June3_2024.xlsx"

xls = pd.ExcelFile(file_path)
sheet_names = xls.sheet_names

print("Total sheets:", len(sheet_names))   # printing the sheet names in the data
print("First 10 sheets:", sheet_names[:10])   # printing first 10 sheets with column names

# Loop through all sheet names in the Excel file
for sheet in sheet_names:

    # Read only the first 5 rows of the current sheet
    df_temp = pd.read_excel(xls, sheet_name=sheet, nrows=5)

    print("\n----------------------------")  # Print a separator for readability

    print("Sheet:", sheet)         # Print the name of the sheet being inspected
    print("Shape:", df_temp.shape)  # Print the shape of the DataFrame (rows, columns)
    print("Columns:", list(df_temp.columns))   # Print the column names in the sheet

# Initialize an empty dictionary to store tag lists per sheet
sheet_tag_summary = {}

# Loop through each sheet again to extract tag IDs
for sheet in sheet_names:
    df_temp = pd.read_excel(xls, sheet_name=sheet, usecols=["motusTagID"])  # Read only the 'motusTagID' column from the sheet
    tags = df_temp["motusTagID"].dropna().unique().tolist()  # Extract unique, non-null tag IDs as a list
    sheet_tag_summary[sheet] = tags  # Store the tag list under the sheet name

sheet_tag_summary   # Display the dictionary containing tag IDs per sheet

# Create a new dictionary including only sheets with more than one tag
multi_owl = {s: t for s, t in sheet_tag_summary.items() if len(t) > 1}

# Show the sheets that contain multiple tag IDs
multi_owl

# Read a specific sheet called "80830" into a DataFrame
df_80830 = pd.read_excel(xls, sheet_name="80830")

# Display the first few rows of the sheet
df_80830.head()

# Get all unique motusTagID values from the sheet "80830"
df_80830["motusTagID"].unique()

# Create a dataframe containing only rows where motusTagID equals 80830
df_owl_80830 = df_80830[df_80830["motusTagID"] == 80830].copy()

# Create a dataframe containing only rows where motusTagID equals 80831
df_owl_80831 = df_80830[df_80830["motusTagID"] == 80831].copy()

# Print the number of rows for each owl/tag ID
print("80830 rows:", len(df_owl_80830))

# Print the number of rows for tag 80831
print("80831 rows:", len(df_owl_80831))

# Print the total number of rows in the original sheet
print("Original 80830 sheet rows:", len(df_80830))

"""We began the exploratory data analysis by inspecting the structure of all 42 sheets in the SawWhets_June3_2024.xlsx file. Each sheet corresponds to one Motus-tagged Northern Saw-whet Owl, except for sheet “80830,” which contained detections from two different owls (80830 and 80831). We identified this by checking the unique motusTagID values inside each sheet. After detecting the mixed sheet, we separated it into two clean datasets based on the tag ID to avoid mixing detections across individuals. We also observed that the first two sheets (80840 and 80839) contained additional antenna port columns (Port #1–Port #4), while most other sheets did not. This difference occurs because these owls were detected at multi-antenna receiver stations. Instead of removing these columns, we kept them to preserve directional signal information. When concatenating all sheets, pandas automatically standardized the columns by aligning shared fields and filling unavailable fields with NaN for owls that lacked them. This produced a unified dataset containing all detections for all owls, ready for further EDA and modeling."""

# Create an empty list to store dataframes from each sheet
all_dfs = []

# Loop through each sheet name in the Excel file
for sheet in sheet_names:
    if sheet == "80830":  # skip the mixed sheet
        continue

    # Read the entire sheet into a temporary dataframe
    temp = pd.read_excel(xls, sheet_name=sheet)
    temp["motusTagID_sheet"] = sheet   # Add a new column labeling which sheet (owl ID) the data came from
    all_dfs.append(temp)  # Store this cleaned dataframe in the list

all_dfs.append(df_owl_80830) # Add the manually separated owl 80830 dataset
all_dfs.append(df_owl_80831)  # Add the manually separated owl 80831 dataset

# Combine every dataframe into one unified dataframe
df_combined = pd.concat(all_dfs, ignore_index=True)

# Print the final number of rows and columns
print("Final combined shape:", df_combined.shape)

# Print how many distinct owl IDs exist based on sheet labels
print("Total unique owls:", df_combined["motusTagID_sheet"].nunique())

# Export the final cleaned dataset to a CSV file
df_combined.to_csv("combined_sawwhet_owls.csv", index=False)

"""“The missing values in the MOTUS dataset are structural rather than random. They arise due to differences in receiver station capabilities (single vs multi-antenna), metadata availability, and export artifacts. Because imputing values (mean/median/KNN) would artificially distort biological patterns and introduce misleading spikes, we retain structural NaNs. Only rows missing essential detection fields (DATE, TIME, SNR, signal) are removed. Tree-based ML models can naturally handle missingness, preserving ecological realism.”"""

# Import visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Set a clean visual theme for all seaborn plots
sns.set(style="whitegrid")

# Numerical columns you want to visualize
num_cols = ["snr", "sig", "noise", "freq"]

# Loop through each numeric column in your list
for col in num_cols:
    if col in df_combined.columns:
        plt.figure(figsize=(10,5))
        sns.histplot(df_combined[col], kde=True, bins=50)
        plt.title(f"Distribution of {col}")
        plt.xlabel(col)
        plt.ylabel("Count")
        plt.show()

# Scatterplot: SNR vs Signal Strength (sig)
plt.figure(figsize=(10,6))

# Scatterplot showing relation between SNR and signal strength
sns.scatterplot(
    data=df_combined,
    x="snr",
    y="sig",
    alpha=0.3
)

# Add title and axis labels
plt.title("SNR vs Signal Strength (sig)")
plt.xlabel("SNR")
plt.ylabel("Signal Strength")
plt.show()

# Bar Chart: # of Detections per Owl (motusTagID)

# create a wide figure for a clear bar chart
plt.figure(figsize=(12,4))

# Count occurrences of each motusTagID and plot as bar chart
df_combined['motusTagID'].value_counts().plot(kind='bar')

# Add title and labels
plt.title("Detections per Owl (motusTagID)")
plt.xlabel("motusTagID")
plt.ylabel("Count")

plt.show()

import pandas as pd
import numpy as np

# Make sure DATE and TIME are proper datetime
df_combined["DATE"] = pd.to_datetime(df_combined["DATE"], errors="coerce")
df_combined["TIME"] = pd.to_datetime(df_combined["TIME"], errors="coerce")

# If TIME already contains full timestamp, use it directly
df_combined["DATETIME"] = df_combined["TIME"]

# Fallback: if some rows don't have TIME but have DATE, use DATE
df_combined["DATETIME"] = df_combined["DATETIME"].fillna(df_combined["DATE"])

# Check how many valid datetimes you have
df_combined["DATETIME"].notna().sum()

# HOURLY DETECTION HISTOGRAM

import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="whitegrid")

# Extract the hour of day from the timestamp column
df_combined["hour"] = df_combined["DATETIME"].dt.hour

# Create a histogram showing detection activity by hour
plt.figure(figsize=(10,5))
sns.histplot(df_combined["hour"], bins=24)

# Add titles and axis labels
plt.title("Hourly Detection Frequency")
plt.xlabel("Hour of Day")
plt.ylabel("Count")
plt.show()

# Extract hour
df_combined["hour"] = df_combined["DATETIME"].dt.hour

# Define night vs day
df_combined["TimeOfDay"] = df_combined["hour"].apply(
    lambda h: "Night" if (h >= 18 or h < 6) else "Day"
)

# DAY vs NIGHT PERCENTAGE BAR CHART
plt.figure(figsize=(8,5))

# Calculate proportional distribution of detections by time-of-day category
(df_combined["TimeOfDay"]
 .value_counts(normalize=True)
 .mul(100)
 .plot(kind="bar", color=["#FFD700", "#1E3A8A"])
)

# Add titles and axis labels
plt.title("Night vs Day Detection Percentage")
plt.xlabel("Time Period")
plt.ylabel("Percentage (%)")

# Display the plot
plt.show()

# Numeric columns you want to sanitize for outliers
numeric_cols = ["snr", "sig", "noise", "freq"]

# Function to cap outliers using IQR-based thresholds
def cap_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_cap = Q1 - 1.5 * IQR
    upper_cap = Q3 + 1.5 * IQR

    # Replace extreme values with boundary caps
    df[col] = df[col].clip(lower_cap, upper_cap)
    return df
df_capped = df_combined.copy()  # Make a safe working copy

# Apply outlier capping to each numeric variable
for col in numeric_cols:
    if col in df_capped.columns:
        df_capped = cap_outliers(df_capped, col)

# Plot BEFORE vs AFTER boxplots for each column

for col in numeric_cols:
    if col in df_capped.columns:

        plt.figure(figsize=(14,5))

        # BEFORE CAPPING
        plt.subplot(1,2,1)
        sns.boxplot(x=df_combined[col])
        plt.title(f"Before Capping: {col}")

        # AFTER CAPPING
        plt.subplot(1,2,2)
        sns.boxplot(x=df_capped[col])
        plt.title(f"After Capping: {col}")

        plt.tight_layout()
        plt.show()

# Prepare list of columns for correlation analysis
num_cols = ["snr", "sig", "sigsd", "noise", "freq", "freqsd", "burstSlop", "slop"]

# Keep only columns that exist in the dataset
num_cols = [c for c in num_cols if c in df_capped.columns]

# Compute correlation matrix
corr = df_capped[num_cols].corr()

corr  # Display the correlation matrix

# Create a new figure with a defined size
plt.figure(figsize=(10,7))

# Draw a heatmap of the correlation matrix
sns.heatmap(corr, annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap (Numeric Signal Features)")
plt.show()

# threshold for dropping
threshold = 0.85

# find columns to drop
corr_matrix = df_capped[num_cols].corr().abs()
upper_tri = corr_matrix.where(
    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
)

to_drop = [
    column for column in upper_tri.columns if any(upper_tri[column] > threshold)
]

to_drop

# Drop unwanted columns from the capped dataframe
df_final = df_capped.drop(columns=to_drop)

print("Dropped columns:", to_drop)  # Print which columns were removed
print("Remaining columns:", df_final.columns.tolist())  # Print remaining columns for verification

# Export the cleaned dataset to a CSV for modeling
df_final.to_csv("cleaned_sawwhet_for_model.csv", index=False)
print("Dropped columns:", to_drop)

# Print shape of the resulting dataset
print("Final Dataset Shape:", df_final.shape)

df_final.head() # Preview the first few rows

# Load your old dataset from file
old_df = pd.read_csv("/content/clean_df_selected.csv")

print(old_df.columns)  # Print column names from the old dataset
print(df_final.columns)  # Print column names from the new cleaned dataset

old_df = pd.read_csv("/content/clean_df_selected.csv")

# make sure tag_id column matches
old_df = old_df.rename(columns={"tag_id": "motusTagID"})

# merge per-owl metadata onto per-detection data
combined_final = df_final.merge(old_df, on="motusTagID", how="left")

# SAVE
combined_final.to_csv("FULL_DATASET.csv", index=False)

combined_final.shape # SHOW SHAPE

"""We started with a multi-sheet Motus dataset containing raw owl detections from 42 receiver stations. Our first step was to combine all sheets into a single dataframe with consistent column names. Once merged, we performed structured EDA to understand missing values, data types, and the uniqueness of each column.

We discovered that most missing values were structural (e.g., receivers with different port numbers), not random errors. Because imputing these values would distort biological signal patterns, we did not apply mean/median/KNN imputation. Instead, we cleaned only essential fields (DATE, TIME, signal values) and kept structural NaNs intact.

Next, we cleaned the timestamp fields by converting DATE and TIME into a unified DATETIME column and created new features like hour of day and Day/Night. We produced key visualizations such as hourly detection frequency, detections per owl, and night-vs-day activity patterns to understand owl movement behaviour.

We then handled outliers using capping (Winsorizing) for signal-based numeric columns to reduce extreme spikes without losing natural biological variation. After that, we performed correlation filtering to drop highly correlated features and avoid multicollinearity.
Finally, we merged the cleaned detection dataset with the metadata dataset (species, age, sex, measurements) using motusTagID, producing a fully enriched dataset ready for modeling.
"""